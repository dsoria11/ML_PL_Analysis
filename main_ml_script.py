import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import os

DATA_FILE = 'quantum_well_data.csv'
DATA_FILE_TYPE = 'csv' # Set to 'csv'

# --- Configuration ---
# IMPORTANT: Adjust these paths and column names to match your actual data!

# 1. Main file with growth parameters for all samples
GROWTH_DATA_FILE = 'growth_data.xlsx'
GROWTH_DATA_FILE_TYPE = 'excel'

# 2. File generated by pl_peak_extractor.py containing PL peak data
EXTRACTED_PEAK_DATA_FILE = 'extracted_pl_peaks.csv'
EXTRACTED_PEAK_DATA_FILE_TYPE = 'csv'

# Define the columns that represent your main input parameters from GROWTH_DATA_FILE
# These names MUST EXACTLY match the column headers from your data.
FEATURE_COLUMNS = ['In_Fraction', 'WellWidth_nm', 'GrowthTemp_Pyro', 'PL_Correction_Factor', 'QW_Temp_Rise_C', 'InGaAs_QW_GrowthRate_ML_Sec']

# Column used to link growth parameters with PL peak data
# This name MUST EXACTLY match the column header in your data file.
SAMPLE_ID_COLUMN = 'QW_Sample'

# Define your ideal/target PL peak (for guidance in exploration)
IDEAL_PL_WAVELENGTH = 1020  # Example: your ideal wavelength in nm (adjust as needed)
IDEAL_PL_INTENSITY = 9.0    # Example: your ideal intensity in a.u. (adjust as needed)

# --- Data loading and preprocessing ---
def load_and_preprocess_data(growth_data_file, growth_data_file_type, extracted_peak_data_file, extracted_peak_file_type):
    # Loads growth parameters from main file, loads extracted PL peak data, and merges them to prepare combined features/targets
    print(f"Loading growth parameter data from {growth_data_file}")
    if growth_data_file == 'csv':
        growth_df = pd.read_csv(growth_data_file)
    elif growth_data_file == 'excel':
        try:
            growth_df = pd.read_excel(growth_data_file, sheet_name=0)
        except ImportError:
            raise ImportError("Please install 'openpyxl' to read .xlsx files: pip install openpyxl")
        except Exception as e:
            print(f"Error reading Excel file: {e}")
            return None, None, None
    else:
        raise ValueError("Unsupported growth_data_file_type. Choose 'csv' or 'excel'.")

    # Validate FEATURE_COLUMNS are present
    missing_features = [col for col in FEATURE_COLUMNS if col not in growth_df.columns]
    if missing_features:
        print(f"Error: Missing FEATURE COLUMNS in '{growth_data_file}': {missing_features}")
        print(f"Available columns: {growth_df.columns.tolist()}")
        return None, None, None

    # Validate SAMPLE_ID_COLUMN is present in growth data
    if SAMPLE_ID_COLUMN not in growth_df.columns:
        print(f"Error: '{SAMPLE_ID_COLUMN}' column not found in '{growth_data_file}'. This column is needed to link PL peaks.")
        return None, None, None

    print(f"Loading extracted PL peaks from {extracted_peak_data_file}...")
    if extracted_peak_file_type=='csv':
        peaks_df = pd.read_csv(extracted_peak_data_file)
    elif extracted_peak_file_type=='excel':
        try:
            peaks_df = pd.read_excel(extracted_peak_data_file, sheet_name='PL_Peaks')
        except ImportError:
            raise ImportError("Please install 'openpyxl' to read .xlsx files: pip install openpyxl")
        except Exception as e:
            print(f"Error reading Excel file: {e}")
            return None, None, None
    else:
        raise ValueError("Unsupported extracted_peak_file_type. Choose 'csv' or 'excel'.")

    # Validate SAMPLE_ID_COLUMN is present in peaks data
    if SAMPLE_ID_COLUMN not in peaks_df.columns:
        print(f"Error: '{SAMPLE_ID_COLUMN}' column not found in '{extracted_peak_data_file}'. Cannot merge.")
        return None, None, None

    # Merge growth parameters with extracted PL peak data
    # Use an inner merge to only keep samples for which we have both growth data and PL peaks
    full_data_df = pd.merge(growth_df, peaks_df, how='inner', on=SAMPLE_ID_COLUMN)

    if full_data_df.empty:
        print("\nError: No samples could be successfully merged. Check if sample IDs match between files.")
        return None, None, None

    # Define features (X) and targets (y) from the merged DataFrame
    X = full_data_df[FEATURE_COLUMNS]
    y = full_data_df[['PL_Peak_Wavelength_nm', 'PL_Peak_Intensity_au']]

    print("\nSample of combined and processed data (features and targets):")
    print(X.head())
    print(y.head())

    return X, y, full_data_df

# --- Model Training ---
def train_model(X_train, y_train, model_type='RandomForest'):
    if model_type == 'RandomForest':
        base_estimator = RandomForestRegressor(n_estimators = 100, random_state=42)
    elif model_type == 'GradientBoosting':
        base_estimator = GradientBoostingRegressor(n_estimators = 100, random_state=42)
    else:
        raise ValueError("Unsupported model_type. Choose 'RandomForest' or 'GradientBoosting'")
    model = MultiOutputRegressor(base_estimator)
    print(f"\nTraining {model_type} Multi-Output Regression Model...")
    model.fit(X_train, y_train)
    print("Model training complete.")
    return model

# --- Model Evaluation ---
def evaluate_model(model, X_test, y_test):
    print("\nEvaluating model performance...")
    y_pred = model.predict(X_test)
    mse_wavelength = mean_squared_error(y_test['PL_Peak_Wavelength_nm'], y_pred[:, 0])
    rmse_wavelength = np.sqrt(mse_wavelength)
    r2_wavelength = r2_score(y_test['PL_Peak_Wavelength_nm'], y_pred[:, 0])
    mse_intensity = mean_squared_error(y_test['PL_Peak_Intensity_au'], y_pred[:, 1])
    rmse_intensity = np.sqrt(mse_intensity)
    r2_intensity = r2_score(y_test['PL_Peak_Intensity_au'], y_pred[:, 1])
    print(f"--- Peak Wavelength Prediction ---")
    print(f"Mean Squared Error (MSE): {mse_wavelength:.2f}")
    print(f"Root Mean Squared Error (RMSE): {rmse_wavelength:.2f}")
    print(f"R-squared (R2): {r2_wavelength:.2f}")
    print(f"\n--- Peak Intensity Prediction ---")
    print(f"Mean Squared Error (MSE): {mse_intensity:.2f}")
    print(f"Root Mean Squared Error (RMSE): {rmse_intensity:.2f}")
    print(f"R-squared (R2): {r2_intensity:.2f}")

    plt.figure(figsize=(14, 7))
    plt.subplot(1, 2, 1)
    plt.scatter(y_test['PL_Peak_Wavelength_nm'], y_pred[:, 0], alpha = 0.7, label='Predicted vs Actual')
    min_val_w = min(y_test['PL_Peak_Wavelength_nm'].min(), y_pred[:, 0].min())
    max_val_w = max(y_test['PL_Peak_Wavelength_nm'].max(), y_pred[:, 0].max())
    plt.plot([min_val_w, max_val_w], [min_val_w, max_val_w], 'r--', label='Prediction')
    plt.xlabel("Actual Peak Wavelength (nm)")
    plt.ylabel("Predicted Peak Wavelength (nm)")
    plt.title("Peak Wavelength: Actual vs Predicted")
    plt.legend()
    plt.grid(True)
    plt.subplot(1, 2, 2)
    plt.scatter(y_test['PL_Peak_Intensity_au'], y_pred[:, 1], alpha = 0.7, label='Predicted vs Actual')
    min_val_i = min(y_test['PL_Peak_Intensity_au'].min(), y_pred[:, 1].min())
    max_val_i = max(y_test['PL_Peak_Intensity_au'].max(), y_pred[:, 1].max())
    plt.plot([min_val_i, max_val_i], [min_val_i, max_val_i], '--r', label='Prediction')
    plt.xlabel("Actual Peak Intensity (a.u.)")
    plt.ylabel("Predicted Peak Intensity (a.u.)")
    plt.title("Peak Intensity: Actual vs Predicted")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# --- Prediction and Guiding Functions ---
def predict_new_recipe(model, in_fraction, well_width_nm, growth_temp_pyro, pl_correction_factor, qw_temp_rise_c, ingaas_qw_growthrate_ml_sec):
    """"
    Predicts PL Peak characteristics for a new quantum well recipe.
    """
    print(f"\n--- Predicting PL for New Recipe ---")

    new_composition_data = {
        'In_Fraction': in_fraction,
        'WellWidth_nm': well_width_nm,
        'GrowthTemp (Pyro)': growth_temp_pyro,
        'PL_Correction_Factor': pl_correction_factor,
        'QW_Temp_Rise_C': qw_temp_rise_c,
        'InGaAs_QW_GrowthRate_ML_Sec': ingaas_qw_growthrate_ml_sec
    }

    new_df = pd.DataFrame([new_composition_data])

    missing_cols = [col for col in FEATURE_COLUMNS if col not in new_df.columns]
    if missing_cols:
        print(f"Error: Missing required feature columns in new recipe data: {missing_cols}")
        print(f"Please ensure all these are present: {FEATURE_COLUMNS}")
        return None, None

    new_X = new_df[FEATURE_COLUMNS]

    try:
        predicted_peaks = model.predict(new_X)
        pred_wavelength = predicted_peaks[0,0]
        pred_intensity = predicted_peaks[0,1]

        print(f"Recipe Input: In_Fraction={in_fraction:.5f}, WellWidth={well_width_nm}nm, GrowthTemp={growth_temp_pyro}°C, ")
        print(f"              PL_Correction_Factor={pl_correction_factor:.4f}, QW_Temp_Rise={qw_temp_rise_c}°C, InGaAs_QW_GrowthRate={ingaas_qw_growthrate_ml_sec:.5f} ML/sec")
        print(f"Predicted PL Peak Wavelength: {pred_wavelength:.2f} nm")
        print(f"Predicted PL Peak Intensity: {pred_intensity:.2f} a.u.")
        return pred_wavelength, pred_intensity
    except Exception as e:
        print(f"An error occurred during prediction: {e}")
        return None, None

    # Also update explore_parameter_space to use the new column names in base_params
def explore_parameter_space(model, base_params, param_to_vary, start_val, end_val, num_steps=10):
    if param_to_vary not in FEATURE_COLUMNS:
        print(f"Error: {param_to_vary} is not in your defined FEATURE_COLUMNS. Cannot vary.")
        print(f"Available features to vary: {FEATURE_COLUMNS}")
        return

    print(f("\n--- Exploring impact of '{param_to_vary}' on PL Peak ---"))
    varied_values = np.linspace(start_val, end_val, num_steps)
    predicted_wavelengths = []
    predicted_intensities = []

    for val in varied_values:
        current_params = base_params.copy()
        current_params[param_to_vary] = val

        # Ensure all FEATURE_COLUMNS are present, filling missing with default values (0 in this case, adapt if needed)
        full_params_for_df = {col: current_params.get(col, 0) for col in FEATURE_COLUMNS}

        new_df = pd.DataFrame([full_params_for_df])
        new_X = new_df[FEATURE_COLUMNS]

        predicted_peaks = model.predict(new_X)
        predicted_wavelengths.append(predicted_peaks[0,0])
        predicted_intensities.append(predicted_peaks[0,1])

    plt.figure(figsize=(14, 6))

    plt.subplot(1, 2, 1)
    plt.plot(varied_values, predicted_wavelengths, marker='o', linestyle='-')
    plt.axhline(y=IDEAL_PL_WAVELENGTH, color='r', linestyle='--', label=f'Ideal Wavelength ({IDEAL_PL_WAVELENGTH} nm)')
    plt.xlabel(param_to_vary)
    plt.ylabel("Predicted Peak Wavelength (nm)")
    plt.title(f"Predicted Wavelength vs {param_to_vary}")
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(varied_values, predicted_intensities, marker='o', linestyle='-', color='orange')
    plt.axhline(y=IDEAL_PL_INTENSITY, color='red', linestyle='--', label=f'Ideal Intensity ({IDEAL_PL_INTENSITY} a.u.)')
    plt.xlabel(param_to_vary)
    plt.ylabel("Predicted Peak Intensity (a.u.)")
    plt.title(f"Predicted Intensity vs {param_to_vary}")
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

# --- Main Execution Flow ---
if __name__ == "__main__":
    print("\n--- Starting Machine Learning Pipeline ---")
    print("This script expects 'growth_parameters.xlsx' and 'extracted_pl_peaks.csv' to exist in the same directory.")
    print("Please ensure these files are properly prepared and in the correct directory before running.")

    # Load and preprocess data using both the growth parameters and extracted peaks
    X, y, full_df = load_and_preprocess_data(GROWTH_DATA_FILE, GROWTH_DATA_FILE_TYPE, EXTRACTED_PEAK_DATA_FILE, EXTRACTED_PEAK_DATA_FILE_TYPE)

    if X is None: # Check if data loading or preprocessing failed
        print("Exiting due to data loading/preprocessing errors.")
    else:
        # Split data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

        print(f"\nTraining set size: {len(X_train)} samples")
        print(f"Test set size: {len(X_test)} samples")

        # Train the model
        model = train_model(X_train, y_train, model_type='RandomForest')

        # Evaluate the model
        evaluate_model(model, X_test, y_test)

        # --- FOCUS: Predicting PL Peak for New Growth Recipes (Trial-and-Error Reduction) ---
        print(f"\n--- Your Current Target: Ideal PL Peak: Wavelength = {IDEAL_PL_WAVELENGTH} nm, Intensity = {IDEAL_PL_INTENSITY} a.u. ---")
        print("\nUse the 'predict_new_recipe()' function to test new growth conditions.")
        print("Modify the values below and re-run the script to test different recipes.")

        # --- Example 1: Based on sample G25-023
        print("\n--- Testing Recipe 1 (Example from G25-023) ---")
        predict_new_recipe(model,
                           in_fraction=0.25301,
                           well_width_nm=8.35,
                           growth_temp_pyro=480,
                           pl_correction_factor=1.0648,
                           qw_temp_rise_c=1.7,
                           ingaas_qw_growthrate_ml_sec=0.46855)

        # --- Example 2: Adjusting PL_Correction_Factor and QW_Temp_Rise_C
        print("\n--- Testing Recipe 2 (Adjusting Correction Factor and Temperature Rise) ---")
        predict_new_recipe(model,
                           in_fraction=0.25301,
                           well_width_nm=8.35,
                           growth_temp_pyro=480,
                           pl_correction_factor=0.9500,
                           qw_temp_rise_c=0,
                           ingaas_qw_growthrate_ml_sec=0.46855)

        # --- Example 3: Adjusting InGaAs Growth Rate
        print("\n--- Testing Recipe 3 (Adjusting InGaAs Growth Rate) ---")
        predict_new_recipe(model,
                           in_fraction=0.25301,
                           well_width_nm=8.35,
                           growth_temp_pyro=480,
                           pl_correction_factor=1.0648,
                           qw_temp_rise_c=1.7,
                           ingaas_qw_growthrate_ml_sec=0.25000)

        # --- Feature Importance ---
        print("\n --- Feature Importance (Relative Impact on Predicted Intensity) ---")
        if isinstance(model.estimator, RandomForestRegressor):
            importances = model.estimator.feature_importances_
            feature_names = FEATURE_COLUMNS
            feature_importance_series = pd.Series(importances, index=feature_names).sort_values(ascending=False)
            print("Feature importances (higher value means more influential):")
            print(feature_importance_series)

            plt.figure(figsize=[10, 6])
            feature_importance_series.plot(kind='bar')
            plt.title('Feature Importances for PL Peak Prediction')
            plt.ylabel('Importance')
            plt.xlabel('Growth Parameter')
            plt.tight_layout()
            plt.show()
        else:
            print("Feature importances are typically available for tree-based models like RandomForestRegressor.")

        # --- Parameter Space Exploration ---
        print("\n--- Running Parameter Space Exploration Simulations ---")
        # Define a base set of parameters. Use values that are commmon or central to your data set.
        # Ensure ALL_FEATURE_COLUMNS are included in base_parameters_for_exploration
        base_parameters_for_exploration = {
            'In_Fraction': 0.25301,
            'WellWidth_nm': 8.35,
            'GrowthTemp (Pyro)': 480,  # Common base temp
            'PL_Correction_Factor': 1.0648,  # Common base factor
            'QW_Temp_Rise_C': 0,  # Common base rise
            'InGaAs_QW_GrowthRate_ML_Sec': 0.46855  # Common base rate
        }

        # Explore the impact of varying the actual growth parameters
        explore_parameter_space(model, base_parameters_for_exploration, param_to_vary='GrowthTemp (Pyro)', start_val=475, end_val=500, num_steps=20)

        explore_parameter_space(model, base_parameters_for_exploration, param_to_vary='PL_Correction_Factor', start_val=0.8, end_val=1.2, num_steps=20)

        explore_parameter_space(model, base_parameters_for_exploration, param_to_vary='QW_Temp_Rise_C', start_val=0, end_val=5, num_steps=10)

        explore_parameter_space(model, base_parameters_for_exploration, param_to_vary='InGaAs_QW_GrowthRate_ML_Sec', start_val=0.2, end_val=0.5, num_steps=15)

        print("\n--- ML Project Complete ---")
        print("By systematically adjusting the 'predict_new_recipe' inputs and analyzing 'explore_parameter_space' plots and Feature Importances,")
        print("you can simulate different growth conditions and iteratively refine your growth recipes to achieve your ideal PL peak.")
        print("Remember: The quality of the model depends entirely on the quality and quantity of your experimental data, especially the VARIATION in input parameters!")